{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок 1. Standalone Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Развернуть standalone cluster Spark: master + 2 workers. Приложить скрипт и/или алгоритм + скрин webui\n",
    "\n",
    "Скрипт в папке spark\n",
    "\n",
    "![image](images/spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Подключиться к кластеру с помощью Jupyter и/или Zeppelin. Приложить скрипт и/или алгоритм + скрин рабочей сессии из инструмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T20:32:29.160240Z",
     "start_time": "2023-04-02T20:32:26.740779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/02 19:55:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Имя хоста Spark Master в docker\n",
    "SPARK_MASTER_HOST = \"spark-master\"\n",
    "# Порт Spark Master\n",
    "SPARK_MASTER_PORT = \"7077\"\n",
    "# Память выделенная для Spark Worker в настройках docker-compose.yml\n",
    "SPARK_WORKER_MEMORY = \"512m\"\n",
    "# Название сессии (любое)\n",
    "SPARK_SESSION = \"pyspark-jupyter\"\n",
    "\n",
    "# Создаем сессию\n",
    "spark = (\n",
    "    SparkSession.builder.appName(SPARK_SESSION)\n",
    "    .master(f\"spark://{SPARK_MASTER_HOST}:{SPARK_MASTER_PORT}\")\n",
    "    .config(\"spark.executor.memory\", SPARK_WORKER_MEMORY)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T20:32:38.362018Z",
     "start_time": "2023-04-02T20:32:37.790597Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://723822d547a6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff77622b60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Развертывание и подключение к HDFS\n",
    "\n",
    "Скрипт развертывания в папке hadoop\n",
    "\n",
    "#### Помещаем файлы которые нужно загрузить в hdfs в папку /hdfs_upload/upload\n",
    "#### выполняем скрипт hdfs_upload.sh\n",
    "\n",
    "![image](images/hadoop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T20:32:45.608717Z",
     "start_time": "2023-04-02T20:32:45.564879Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Имя Hadoop Namenode в docker\n",
    "HADOOP_HOST = 'hadoop-namenode'\n",
    "# Hadoop Namenode port\n",
    "HADOOP_PORT = \"9000\"\n",
    "# Hadoop Namenode port\n",
    "HADOOP_WEB_PORT = \"9870\"\n",
    "# Hadoop default directory\n",
    "HADOOP_DIR = \"upload\"\n",
    "\n",
    "\n",
    "def hdfs_files(\n",
    "    host: str = HADOOP_HOST,\n",
    "    port: str = HADOOP_WEB_PORT,\n",
    "    directory: str = HADOOP_DIR,\n",
    "    f_filter: str = \"\",\n",
    "    f_extension: str = \"\",\n",
    ") -> list | str:\n",
    "    \"\"\"Returns files list of target directory.\n",
    "\n",
    "    :param host: hadoop namenode host name\n",
    "    :param port: hadoop namenode web port\n",
    "    :param directory: target directory to list.\n",
    "    :param f_filter: string to filter filename with\n",
    "    :param f_extension: file extension ('csv', 'parquet', etc..)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"http://{host}:{port}/webhdfs/v1/{directory}?op=LISTSTATUS\"\n",
    "        ).json()\n",
    "    except Exception as error:\n",
    "        return error\n",
    "\n",
    "    if response.get(\"FileStatuses\"):\n",
    "        files_data = response[\"FileStatuses\"].get(\"FileStatus\")\n",
    "        file_list = [\n",
    "            file.get(\"pathSuffix\")\n",
    "            for file in files_data\n",
    "            if f_filter in file.get(\"pathSuffix\")\n",
    "            and file.get(\"pathSuffix\").endswith(f_extension)\n",
    "            and file.get(\"type\") == \"FILE\"\n",
    "        ]\n",
    "        return file_list\n",
    "    elif response.get(\"RemoteException\"):\n",
    "        return response[\"RemoteException\"].get(\"message\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок 2. Работа с данными на Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Преобразовать данные исходного датасета в parquet объединяя все таблицы. Оценить разницу в скорости чтения / занимаемом объеме. Сделать выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получаем список файлов датасета book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T20:32:49.601374Z",
     "start_time": "2023-04-02T20:32:48.626978Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book1-100k.csv',\n",
       " 'book1000k-1100k.csv',\n",
       " 'book100k-200k.csv',\n",
       " 'book1100k-1200k.csv',\n",
       " 'book1200k-1300k.csv',\n",
       " 'book1300k-1400k.csv',\n",
       " 'book1400k-1500k.csv',\n",
       " 'book1500k-1600k.csv',\n",
       " 'book1600k-1700k.csv',\n",
       " 'book1700k-1800k.csv',\n",
       " 'book1800k-1900k.csv',\n",
       " 'book1900k-2000k.csv',\n",
       " 'book2000k-3000k.csv',\n",
       " 'book200k-300k.csv',\n",
       " 'book3000k-4000k.csv',\n",
       " 'book300k-400k.csv',\n",
       " 'book4000k-5000k.csv',\n",
       " 'book400k-500k.csv',\n",
       " 'book500k-600k.csv',\n",
       " 'book600k-700k.csv',\n",
       " 'book700k-800k.csv',\n",
       " 'book800k-900k.csv',\n",
       " 'book900k-1000k.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_files = hdfs_files(f_filter='book', f_extension='csv')\n",
    "book_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "#### Так как схема в разных файлах может различаться берем схему из первого файла и создаем пустой датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Ссылка доступа к HDFS\n",
    "HADOOP_LINK = f\"hdfs://{HADOOP_HOST}:{HADOOP_PORT}\"\n",
    "\n",
    "schema = spark.read.csv(\n",
    "    f\"{HADOOP_LINK}/{HADOOP_DIR}/{book_files[0]}\", header=True, inferSchema=True\n",
    ").limit(1).schema\n",
    "\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "spark_df = spark.createDataFrame(data = emp_RDD, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, Name: string, RatingDist1: string, pagesNumber: string, RatingDist4: string, RatingDistTotal: string, PublishMonth: string, PublishDay: string, Publisher: string, CountsOfReview: string, PublishYear: string, Language: string, Authors: string, Rating: string, RatingDist2: string, RatingDist5: string, ISBN: string, RatingDist3: string]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Читаем каждый файл и объединяем в один датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for file in book_files:\n",
    "    \n",
    "    spark_data = (\n",
    "        spark.read\n",
    "        .option(\"multiline\", \"true\")\n",
    "        .option(\"quote\", '\"')\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"escape\", \"\\\\\")\n",
    "        .option(\"escape\", '\"')\n",
    "        .csv(f\"{HADOOP_LINK}/{HADOOP_DIR}/{file}\")\n",
    "    )\n",
    "    # spark_data = spark.read.csv(f\"{HADOOP_LINK}/{HADOOP_DIR}/{file}\", header=True, multiLine=True)\n",
    "    spark_df = spark_df.unionByName(spark_data, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаляем дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df = spark_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|summary|pagesNumber|\n",
      "+-------+-----------+\n",
      "|  count|          0|\n",
      "|   mean|       null|\n",
      "| stddev|       null|\n",
      "|    min|       null|\n",
      "|    max|       null|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Записываем dataframe parquet в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "FILENAME = 'book.parquet'\n",
    "\n",
    "write_hadoop = (\n",
    "    spark_df.write.option(\"header\", True)\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(f\"hdfs://{HADOOP_HOST}:{HADOOP_PORT}/{FILENAME}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Читаем dataframe из HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_load = spark.read.parquet(f\"hdfs://{HADOOP_HOST}:{HADOOP_PORT}/{FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Сравниваем время выполнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_time(command):\n",
    "    start_time = time.time()\n",
    "    command\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------\n",
      " Id                    | 400000               \n",
      " Name                  | The Gigli Concert    \n",
      " RatingDist1           | 1:2                  \n",
      " pagesNumber           | 96                   \n",
      " RatingDist4           | 4:10                 \n",
      " RatingDistTotal       | total:27             \n",
      " PublishMonth          | 16                   \n",
      " PublishDay            | 9                    \n",
      " Publisher             | Bloomsbury Methue... \n",
      " CountsOfReview        | 2                    \n",
      " PublishYear           | 1991                 \n",
      " Language              | null                 \n",
      " Authors               | Tom    Murphy        \n",
      " Rating                | 3.3                  \n",
      " RatingDist2           | 2:4                  \n",
      " RatingDist5           | 5:3                  \n",
      " ISBN                  | 0413659305           \n",
      " RatingDist3           | 3:8                  \n",
      " Description           | null                 \n",
      " Count of text reviews | null                 \n",
      "\n",
      "--- 9.298324584960938e-06 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# CSV\n",
    "\n",
    "check_time(\n",
    "    spark_df.filter(spark_df['Id'] == '400000').show(vertical=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------\n",
      " Id                    | 400000               \n",
      " Name                  | The Gigli Concert    \n",
      " RatingDist1           | 1:2                  \n",
      " pagesNumber           | 96                   \n",
      " RatingDist4           | 4:10                 \n",
      " RatingDistTotal       | total:27             \n",
      " PublishMonth          | 16                   \n",
      " PublishDay            | 9                    \n",
      " Publisher             | Bloomsbury Methue... \n",
      " CountsOfReview        | 2                    \n",
      " PublishYear           | 1991                 \n",
      " Language              | null                 \n",
      " Authors               | Tom    Murphy        \n",
      " Rating                | 3.3                  \n",
      " RatingDist2           | 2:4                  \n",
      " RatingDist5           | 5:3                  \n",
      " ISBN                  | 0413659305           \n",
      " RatingDist3           | 3:8                  \n",
      " Description           | null                 \n",
      " Count of text reviews | null                 \n",
      "\n",
      "--- 3.337860107421875e-06 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Parquet\n",
    "\n",
    "check_time(\n",
    "    df_load.filter(df_load['Id'] == '400000').show(vertical=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Сравниваем объем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hdfs_dir_size(\n",
    "    host: str = HADOOP_HOST,\n",
    "    port: str = HADOOP_WEB_PORT,\n",
    "    directory: str = HADOOP_DIR,\n",
    "    f_filter: str = \"\",\n",
    "    f_extension: str = \"\",\n",
    ") -> list | str:\n",
    "    \"\"\"\n",
    "    Return size of directory in Mb.\n",
    "\n",
    "    :param host: hadoop namenode host name\n",
    "    :param port: hadoop namenode web port\n",
    "    :param directory: target directory to list.\n",
    "    :param f_filter: string to filter filename with\n",
    "    :param f_extension: file extension ('csv', 'parquet', etc..)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"http://{host}:{port}/webhdfs/v1/{directory}?op=LISTSTATUS\"\n",
    "        ).json()\n",
    "    except Exception as error:\n",
    "        return error\n",
    "\n",
    "    if response.get(\"FileStatuses\"):\n",
    "        files_data = response[\"FileStatuses\"].get(\"FileStatus\")\n",
    "        files_size = [\n",
    "            int(file.get(\"length\"))\n",
    "            for file in files_data\n",
    "            if f_filter in file.get(\"pathSuffix\")\n",
    "            and file.get(\"pathSuffix\").endswith(f_extension)\n",
    "            and file.get(\"type\") == \"FILE\"\n",
    "        ]\n",
    "        return sum(files_size) / 1048576\n",
    "    elif response.get(\"RemoteException\"):\n",
    "        return response[\"RemoteException\"].get(\"message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1110.6515398025513"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV files size\n",
    "\n",
    "hdfs_dir_size(directory=\"upload\", f_filter=\"book\", f_extension=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692.379002571106"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parquet files size\n",
    "\n",
    "hdfs_dir_size(directory=\"book.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Используя весь набор данных с помощью Spark вывести"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Топ-10 книг с наибольшим числом ревью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum, avg, max, min, mean, count\n",
    "\n",
    "# Изменяем тип столбца CountsOfReview на int\n",
    "df_load = df_load.withColumn(\"CountsOfReview\", df_load.CountsOfReview.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 141:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------------+--------------+\n",
      "|Id     |Name                                                     |CountsOfReview|\n",
      "+-------+---------------------------------------------------------+--------------+\n",
      "|2767052|The Hunger Games (The Hunger Games, #1)                  |154447        |\n",
      "|41865  |Twilight (Twilight, #1)                                  |94850         |\n",
      "|19063  |The Book Thief                                           |87685         |\n",
      "|4667024|The Help                                                 |76040         |\n",
      "|3      |Harry Potter and the Sorcerer's Stone (Harry Potter, #1) |75911         |\n",
      "|3636   |The Giver (The Giver, #1)                                |57034         |\n",
      "|43641  |Water for Elephants                                      |52918         |\n",
      "|2429135|The Girl with the Dragon Tattoo (Millennium, #1)         |52225         |\n",
      "|136251 |Harry Potter and the Deathly Hallows (Harry Potter, #7)  |52088         |\n",
      "|28187  |The Lightning Thief (Percy Jackson and the Olympians, #1)|48630         |\n",
      "+-------+---------------------------------------------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_10_book = (\n",
    "    df_load\n",
    "    .orderBy('CountsOfReview', ascending=False)\n",
    "    .select(\"Id\", \"Name\", \"CountsOfReview\")\n",
    "    .limit(10)\n",
    ")\n",
    "top_10_book.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Топ-10 издателей с наибольшим средним числом страниц в книгах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Изменяем тип столбца pagesNumber на int\n",
    "df_load = df_load.withColumn(\"pagesNumber\", df_load.pagesNumber.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:===================================>                     (5 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+------------------+\n",
      "|Publisher                                                  |average_pages     |\n",
      "+-----------------------------------------------------------+------------------+\n",
      "|Crafty Secrets Publications                                |1807321.6         |\n",
      "|Sacred-texts.com                                           |500000.0          |\n",
      "|Department of Russian Language and Literature University of|322128.5714285714 |\n",
      "|Logos Research Systems                                     |100000.0          |\n",
      "|Encyclopedia Britannica, Incorporated                      |32642.0           |\n",
      "|Progressive Management                                     |19106.3625        |\n",
      "|Still Waters Revival Books                                 |10080.142857142857|\n",
      "|P. Shalom Publications, Incorporated                       |8539.0            |\n",
      "|Hendrickson Publishers, Inc. (Peabody, MA)                 |6448.0            |\n",
      "|IEEE/EMB                                                   |6000.0            |\n",
      "+-----------------------------------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, max, min, mean, count\n",
    "\n",
    "top_10_pub = (\n",
    "    df_load.groupBy(\"Publisher\")\n",
    "    .agg(avg(\"pagesNumber\")\n",
    "    .alias(\"average_pages\"))\n",
    "    .sort('average_pages', ascending=False)\n",
    "    .limit(10)\n",
    ")\n",
    "top_10_pub.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       pagesNumber|\n",
      "+-------+------------------+\n",
      "|  count|           1850198|\n",
      "|   mean|276.55174202977196|\n",
      "| stddev| 5006.170699333496|\n",
      "|    min|                 0|\n",
      "|    max|           4517845|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load.describe(['pagesNumber']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:46:45.195248Z",
     "start_time": "2023-03-31T14:46:44.717837Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
